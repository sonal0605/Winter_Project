{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport re\nimport nltk\nnltk.download('omw-1.4')\nimport spacy\nimport string\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import word_tokenize,sent_tokenize # tokenizing\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nimport nltk\nfrom nltk.stem import PorterStemmer\n#stop-words\nstop_words=set(nltk.corpus.stopwords.words('english'))\n# Graphs\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\n\n# vectorizers for creating the document-term-matrix (DTM)\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.metrics import accuracy_score,roc_auc_score,f1_score,classification_report,precision_score, recall_score\nfrom mlxtend.plotting import plot_confusion_matrix\n# ML Models\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom xgboost import XGBClassifier\n#keras #tf\nimport tensorflow as tf\nimport keras\nfrom keras.preprocessing.text import one_hot,Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM, BatchNormalization, Dropout, InputLayer, ReLU\nfrom keras.initializers import Constant\nfrom keras.callbacks import EarlyStopping\nimport keras_tuner as kt\nimport tensorflow_addons as tfa\n\nF1_Score_tf = tfa.metrics.F1Score(num_classes=1,average=\"weighted\")\nauc_roc_metric_ROC = keras.metrics.AUC(curve='ROC')\n\n\n#gensim w2v  #word2vec\nimport gensim\nfrom gensim.models import Word2Vec,KeyedVectors\nfrom nltk.tokenize import sent_tokenize, word_tokenize","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-30T10:09:35.626149Z","iopub.execute_input":"2022-12-30T10:09:35.626654Z","iopub.status.idle":"2022-12-30T10:09:35.654331Z","shell.execute_reply.started":"2022-12-30T10:09:35.6266Z","shell.execute_reply":"2022-12-30T10:09:35.652754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"# data = pd.DataFrame()\n# for x in os.listdir(\"/kaggle/input/cyberbullying-dataset\"):\n#     if x.endswith(\".csv\"):\n#         # Prints only text file present in My Folder\n#         print(x)\n#         temp = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/\"+x)\n#         data = pd.concat([data,temp])","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:09:35.656604Z","iopub.execute_input":"2022-12-30T10:09:35.656991Z","iopub.status.idle":"2022-12-30T10:09:35.662407Z","shell.execute_reply.started":"2022-12-30T10:09:35.656957Z","shell.execute_reply":"2022-12-30T10:09:35.660877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_1 = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/aggression_parsed_dataset.csv\")\n# data_1.drop(columns=[\"index\",\"ed_label_0\",\"ed_label_1\"],inplace=True)\n# data_2 = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/attack_parsed_dataset.csv\")\n# data_2.drop(columns=[\"index\",\"ed_label_0\",\"ed_label_1\"],inplace=True)\n# data_3 = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/kaggle_parsed_dataset.csv\")\n# data_3.drop(columns=[\"index\",\"Date\"],inplace=True)\n# data_3 = data_3[['Text','oh_label']] \n# data_4 = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/toxicity_parsed_dataset.csv\")\n# data_4.drop(columns=[\"index\",\"ed_label_0\",\"ed_label_1\"],inplace=True)\n# data_5 = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/twitter_parsed_dataset.csv\")\n# data_5.drop(columns=[\"index\",\"id\",\"Annotation\"],inplace=True)\n\n# data_6 = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/twitter_racism_parsed_dataset.csv\")\n# data_6.drop(columns=[\"index\",\"id\",\"Annotation\"],inplace=True)\n\n# data_7 = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/twitter_sexism_parsed_dataset.csv\")\n# data_7.drop(columns=[\"index\",\"id\",\"Annotation\"],inplace=True)\n\n# data_8 = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/youtube_parsed_dataset.csv\")\n# data_8.drop(columns=[\"index\",\"UserIndex\",\"Number of Comments\",\"Number of Subscribers\",\"Membership Duration\",\n#                         \"Number of Uploads\",\"Profanity in UserID\",\"Age\"],inplace=True)\n\n# data = pd.concat([data_1,data_2,data_3,data_4,data_5,data_6,data_7,data_8])\n# data.reset_index(inplace=True)\n# data.drop(columns=['index'],inplace=True)\n\ndata = pd.read_csv(\"/kaggle/input/cyberbullying-dataset/twitter_parsed_dataset.csv\")\ndata.dropna(inplace=True)\ndata.reset_index(inplace=True,drop=True)\ndata.drop(columns=['index','id','Annotation'],inplace=True)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:09:35.665138Z","iopub.execute_input":"2022-12-30T10:09:35.665667Z","iopub.status.idle":"2022-12-30T10:09:35.765968Z","shell.execute_reply.started":"2022-12-30T10:09:35.665619Z","shell.execute_reply":"2022-12-30T10:09:35.764758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count number of characters \ndef count_chars(text):\n    return len(text)\n\n# count number of words \ndef count_words(text):\n    return len(text.split())\n\n# count number of capital characters\ndef count_capital_chars(text):\n    count=0\n    for i in text:\n        if i.isupper():\n            count+=1\n    return count\n\n# count number of capital words\ndef count_capital_words(text):\n    return sum(map(str.isupper,text.split()))\n\n# count number of punctuations\ndef count_punctuations(text):\n    punctuations='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n    d=0\n    for i in punctuations:\n        d = d + text.count(i)\n    return d\n\n# count number of words in quotes\ndef count_words_in_quotes(text):\n    x = re.findall(\"\\'.\\'|\\\".\\\"\", text)\n    count=0\n    if x is None:\n        return 0\n    else:\n        for i in x:\n            t=i[1:-1]\n            count+=count_words(t)\n        return count\n    \n# count number of sentences\ndef count_sent(text):\n    return len(nltk.sent_tokenize(text))\n\n# calculate average sentence length\ndef avg_sent_len(word_cnt,sent_cnt):\n    return word_cnt/sent_cnt\n\n# count number of unique words \ndef count_unique_words(text):\n    return len(set(text.split()))\n            \n# words vs unique feature\ndef words_vs_unique(words,unique):\n    return unique/words\n    \n# count of hashtags\ndef count_htags(text):\n    x = re.findall(r'(\\#\\w[A-Za-z0-9]*)', text)\n    return len(x)\n\n# count of mentions\ndef count_mentions(text):\n    x = re.findall(r'(\\@\\w[A-Za-z0-9]*)', text)\n    return len(x)\n\n# count of stopwords\ndef count_stopwords(text):\n    stop_words = set(stopwords.words('english'))  \n    word_tokens = word_tokenize(text)\n    stopwords_x = [w for w in word_tokens if w in stop_words]\n    return len(stopwords_x)\n\n# stopwords vs words\ndef stopwords_vs_words(stopwords_cnt,text):\n    return stopwords_cnt/len(word_tokenize(text))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:09:35.767689Z","iopub.execute_input":"2022-12-30T10:09:35.76804Z","iopub.status.idle":"2022-12-30T10:09:35.783229Z","shell.execute_reply.started":"2022-12-30T10:09:35.768009Z","shell.execute_reply":"2022-12-30T10:09:35.781772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['char_count'] = data[\"Text\"].apply(lambda x:count_chars(str(x)))\ndata['word_count'] = data[\"Text\"].apply(lambda x:count_words(str(x)))\ndata['sent_count'] = data[\"Text\"].apply(lambda x:count_sent(str(x)))\ndata['capital_char_count'] = data[\"Text\"].apply(lambda x:count_capital_chars(str(x)))\ndata['capital_word_count'] = data[\"Text\"].apply(lambda x:count_capital_words(str(x)))\ndata['quoted_word_count'] = data[\"Text\"].apply(lambda x:count_words_in_quotes(str(x)))\ndata['stopword_count'] = data[\"Text\"].apply(lambda x:count_stopwords(str(x)))\ndata['unique_word_count'] = data[\"Text\"].apply(lambda x:count_unique_words(str(x)))\ndata['htag_count'] = data[\"Text\"].apply(lambda x:count_htags(str(x)))\ndata['mention_count'] = data[\"Text\"].apply(lambda x:count_mentions(str(x)))\ndata['punct_count'] = data[\"Text\"].apply(lambda x:count_punctuations(str(x)))\ndata['avg_wordlength']=data['char_count']/data['word_count']\ndata['avg_sentlength']=data['word_count']/data['sent_count']\ndata['unique_vs_words']=data['unique_word_count']/data['word_count']\ndata['stopwords_vs_words']=data['stopword_count']/data['word_count']","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:09:35.786423Z","iopub.execute_input":"2022-12-30T10:09:35.786969Z","iopub.status.idle":"2022-12-30T10:09:44.751911Z","shell.execute_reply.started":"2022-12-30T10:09:35.786923Z","shell.execute_reply":"2022-12-30T10:09:44.750665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Data Pre-Processing and Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"Reference: <br>\nhttps://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing#Removal-of-Emojis\nhttps://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py","metadata":{}},{"cell_type":"code","source":"data[\"Text\"][145:155].values","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:09:44.753425Z","iopub.execute_input":"2022-12-30T10:09:44.753831Z","iopub.status.idle":"2022-12-30T10:09:44.762078Z","shell.execute_reply.started":"2022-12-30T10:09:44.753799Z","shell.execute_reply":"2022-12-30T10:09:44.760743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"Text\"][2550:2560].values","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:09:44.763535Z","iopub.execute_input":"2022-12-30T10:09:44.763951Z","iopub.status.idle":"2022-12-30T10:09:44.774977Z","shell.execute_reply.started":"2022-12-30T10:09:44.763909Z","shell.execute_reply":"2022-12-30T10:09:44.773827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - Data contains some Emojis and Emoticons. Also data is obtained from social media, presence of chatwords are highly likely.","metadata":{}},{"cell_type":"markdown","source":"Will replace these with words","metadata":{}},{"cell_type":"code","source":"import pickle\nwith open('/kaggle/input/emoji-dictionary-1/Emoji_Dict.p', 'rb') as fp:\n    Emoji_Dict = pickle.load(fp)\nEmoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n\nwith open('/kaggle/input/emotions-dictionary-for-nlp/Emoticon_Dict.p', 'rb') as fp:\n    Emoticon_Dict = pickle.load(fp)\nEmoticon_Dict = {v: k for k, v in Emoticon_Dict.items()}\n\ndef convert_emojis_to_word(text):\n    for emot in Emoji_Dict:\n        text = re.sub(r'('+emot+')', \"_\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n    return text\n\ndef convert_emoticons(text):\n    for emot in Emoticon_Dict:\n        text = re.sub(u'('+emot+')', \"_\".join(Emoticon_Dict[emot].replace(\",\",\"\").split()), text)\n    return text\n\n# text = \"I won ü•á in üèè\"\n# convert_emojis_to_word(text)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:09:44.777042Z","iopub.execute_input":"2022-12-30T10:09:44.777533Z","iopub.status.idle":"2022-12-30T10:09:44.810924Z","shell.execute_reply.started":"2022-12-30T10:09:44.777488Z","shell.execute_reply":"2022-12-30T10:09:44.809907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ntqdm.pandas(desc=\"my bar!\")\n\ncolumn = \"Text\"\ndata[column] = data[column].progress_apply(lambda text: convert_emoticons(str(text)))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:09:44.8143Z","iopub.execute_input":"2022-12-30T10:09:44.814689Z","iopub.status.idle":"2022-12-30T10:09:47.831524Z","shell.execute_reply.started":"2022-12-30T10:09:44.814654Z","shell.execute_reply":"2022-12-30T10:09:47.830195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[column] = data[column].progress_apply(lambda text: convert_emojis_to_word(str(text)))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:09:47.832995Z","iopub.execute_input":"2022-12-30T10:09:47.833624Z","iopub.status.idle":"2022-12-30T10:48:29.400199Z","shell.execute_reply.started":"2022-12-30T10:09:47.833585Z","shell.execute_reply":"2022-12-30T10:48:29.399009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print( data[\"Text\"][145:155].values )\nprint( data[\"Text\"][2550:2560].values )","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:29.401919Z","iopub.execute_input":"2022-12-30T10:48:29.402369Z","iopub.status.idle":"2022-12-30T10:48:29.415113Z","shell.execute_reply.started":"2022-12-30T10:48:29.402327Z","shell.execute_reply":"2022-12-30T10:48:29.413901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Remove Emoji and Emoticons\n\n# column = \"Text\"\n# def remove_emoji(string):\n#     emoji_pattern = re.compile(\"[\"\n#                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n#                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n#                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n#                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n#                            u\"\\U00002702-\\U000027B0\"\n#                            u\"\\U000024C2-\\U0001F251\"\n#                            \"]+\", flags=re.UNICODE)\n#     return emoji_pattern.sub(r'', string)\n\n# def remove_emoticons(text):\n#     emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n#     return emoticon_pattern.sub(r'', text)\n\n# data[column] = data[column].apply(lambda text: remove_emoji(str(text)))\n# data[column] = data[column].apply(lambda text: remove_emoticons(str(text)))\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:29.417283Z","iopub.execute_input":"2022-12-30T10:48:29.418004Z","iopub.status.idle":"2022-12-30T10:48:29.426674Z","shell.execute_reply.started":"2022-12-30T10:48:29.417967Z","shell.execute_reply":"2022-12-30T10:48:29.425486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_copy = data.copy()\ndata_copy","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:29.428402Z","iopub.execute_input":"2022-12-30T10:48:29.428735Z","iopub.status.idle":"2022-12-30T10:48:29.46855Z","shell.execute_reply.started":"2022-12-30T10:48:29.428706Z","shell.execute_reply":"2022-12-30T10:48:29.467219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data_copy.copy()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:29.469891Z","iopub.execute_input":"2022-12-30T10:48:29.470373Z","iopub.status.idle":"2022-12-30T10:48:29.476739Z","shell.execute_reply.started":"2022-12-30T10:48:29.470325Z","shell.execute_reply":"2022-12-30T10:48:29.475441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:29.4784Z","iopub.execute_input":"2022-12-30T10:48:29.478895Z","iopub.status.idle":"2022-12-30T10:48:29.5073Z","shell.execute_reply.started":"2022-12-30T10:48:29.478833Z","shell.execute_reply":"2022-12-30T10:48:29.506093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove URLs and HTML tags\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndef remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ndata[column] = data[column].apply(lambda text: remove_urls(str(text)))\ndata[column] = data[column].apply(lambda text: remove_html(str(text)))\ndata","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:29.508683Z","iopub.execute_input":"2022-12-30T10:48:29.509458Z","iopub.status.idle":"2022-12-30T10:48:29.605956Z","shell.execute_reply.started":"2022-12-30T10:48:29.509419Z","shell.execute_reply":"2022-12-30T10:48:29.604858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chat_words_str = \"\"\"\nAFAIK=As Far As I Know\nAFK=Away From Keyboard\nASAP=As Soon As Possible\nATK=At The Keyboard\nATM=At The Moment\nA3=Anytime, Anywhere, Anyplace\nBAK=Back At Keyboard\nBBL=Be Back Later\nBBS=Be Back Soon\nBFN=Bye For Now\nB4N=Bye For Now\nBRB=Be Right Back\nBRT=Be Right There\nBTW=By The Way\nB4=Before\nB4N=Bye For Now\nCU=See You\nCUL8R=See You Later\nCYA=See You\nFAQ=Frequently Asked Questions\nFC=Fingers Crossed\nFWIW=For What It's Worth\nFYI=For Your Information\nGAL=Get A Life\nGG=Good Game\nGN=Good Night\nGMTA=Great Minds Think Alike\nGR8=Great!\nG9=Genius\nIC=I See\nICQ=I Seek you (also a chat program)\nILU=ILU: I Love You\nIMHO=In My Honest/Humble Opinion\nIMO=In My Opinion\nIOW=In Other Words\nIRL=In Real Life\nKISS=Keep It Simple, Stupid\nLDR=Long Distance Relationship\nLMAO=Laugh My A.. Off\nLOL=Laughing Out Loud\nLTNS=Long Time No See\nL8R=Later\nMTE=My Thoughts Exactly\nM8=Mate\nNRN=No Reply Necessary\nOIC=Oh I See\nPITA=Pain In The A..\nPRT=Party\nPRW=Parents Are Watching\nQPSA?=Que Pasa?\nROFL=Rolling On The Floor Laughing\nROFLOL=Rolling On The Floor Laughing Out Loud\nROTFLMAO=Rolling On The Floor Laughing My A.. Off\nSK8=Skate\nSTATS=Your sex and age\nASL=Age, Sex, Location\nTHX=Thank You\nTTFN=Ta-Ta For Now!\nTTYL=Talk To You Later\nU=You\nU2=You Too\nU4E=Yours For Ever\nWB=Welcome Back\nWTF=What The F...\nWTG=Way To Go!\nWUF=Where Are You From?\nW8=Wait...\n7K=Sick:-D Laugher\n\"\"\"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-12-30T10:48:29.607218Z","iopub.execute_input":"2022-12-30T10:48:29.607536Z","iopub.status.idle":"2022-12-30T10:48:29.614509Z","shell.execute_reply.started":"2022-12-30T10:48:29.607508Z","shell.execute_reply":"2022-12-30T10:48:29.613259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert Chats Words\n\nchat_words_map_dict = {}\nchat_words_list = []\nfor line in chat_words_str.split(\"\\n\"):\n    if line != \"\":\n        cw = line.split(\"=\")[0]\n        cw_expanded = line.split(\"=\")[1]\n        chat_words_list.append(cw)\n        chat_words_map_dict[cw] = cw_expanded\nchat_words_list = set(chat_words_list)\n\ndef chat_words_conversion(text):\n    new_text = []\n    for w in text.split():\n        if w.upper() in chat_words_list:\n            new_text.append(chat_words_map_dict[w.upper()])\n        else:\n            new_text.append(w)\n    return \" \".join(new_text)\n\ndata[column] = data[column].progress_apply(lambda text: chat_words_conversion(str(text)))\ndata","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:29.616271Z","iopub.execute_input":"2022-12-30T10:48:29.616661Z","iopub.status.idle":"2022-12-30T10:48:29.788569Z","shell.execute_reply.started":"2022-12-30T10:48:29.61662Z","shell.execute_reply":"2022-12-30T10:48:29.787398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_numbers(text):\n    number_pattern = r'\\d+'\n    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n    return without_number\n\ndata[column] = data[column].progress_apply(lambda text: remove_numbers(str(text)))\ndata","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:29.790114Z","iopub.execute_input":"2022-12-30T10:48:29.790456Z","iopub.status.idle":"2022-12-30T10:48:29.91418Z","shell.execute_reply.started":"2022-12-30T10:48:29.790427Z","shell.execute_reply":"2022-12-30T10:48:29.912894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_lowercase(data,column):\n    data[column] = data[column].str.lower()\n    return data\ndata = make_lowercase(data,column='Text')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:29.920312Z","iopub.execute_input":"2022-12-30T10:48:29.920677Z","iopub.status.idle":"2022-12-30T10:48:29.938909Z","shell.execute_reply.started":"2022-12-30T10:48:29.920646Z","shell.execute_reply":"2022-12-30T10:48:29.93764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import TextBlob\ndata[\"sentiment\"] = data[\"Text\"].progress_apply(lambda x: TextBlob(x).sentiment.polarity)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:29.940741Z","iopub.execute_input":"2022-12-30T10:48:29.941491Z","iopub.status.idle":"2022-12-30T10:48:34.637538Z","shell.execute_reply.started":"2022-12-30T10:48:29.94144Z","shell.execute_reply":"2022-12-30T10:48:34.636379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_words = set(['no',\"wasn't\",\"has't\",\"didn't\",\"hadn't\",\"mightn't\",\"needn't\",\"won't\",\"weren't\",\"doesn't\",\"mustn't\",\"shouldn't\",\"haven't\",\"doesn\",\"hadn\",\"didn't\",\"not\",\"needn\",\"aren't\",\"isn\",\"aren\",\"isn't\",\"wasn\",\"hasn't\",\"didn\",\"shouldn\",\"don't\",\"couldn't\",\"wouldn't\",\"weren\",\"hasn\",\"\"])\nstop_words_n = set(stopwords.words('english'))-no_words\n\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in stop_words_n])\n\ndata[\"Text\"] = data[\"Text\"].progress_apply(lambda text: remove_stopwords(text))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:34.639092Z","iopub.execute_input":"2022-12-30T10:48:34.639679Z","iopub.status.idle":"2022-12-30T10:48:34.745048Z","shell.execute_reply.started":"2022-12-30T10:48:34.639646Z","shell.execute_reply":"2022-12-30T10:48:34.743707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_AtSign_tags(data,column):\n    data[column] = data[column].apply( lambda text: re.sub('\\@[a-zA-Z]+','',text) )\n    return data\n\ndata = remove_AtSign_tags(data,column='Text')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:34.746297Z","iopub.execute_input":"2022-12-30T10:48:34.74664Z","iopub.status.idle":"2022-12-30T10:48:34.7854Z","shell.execute_reply.started":"2022-12-30T10:48:34.746608Z","shell.execute_reply":"2022-12-30T10:48:34.784229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_puntuations(data,column):\n    data[column] = data[column].progress_apply(lambda text: str(text).translate(str.maketrans('','',string.punctuation)))\n    return data\ndata = remove_puntuations(data,\"Text\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:34.78673Z","iopub.execute_input":"2022-12-30T10:48:34.787217Z","iopub.status.idle":"2022-12-30T10:48:34.908387Z","shell.execute_reply.started":"2022-12-30T10:48:34.787179Z","shell.execute_reply":"2022-12-30T10:48:34.90711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Remove single characters\n\ndef remove_single_char(text):\n    single_char_pattern = r'\\s+[a-zA-Z]\\s+'\n    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n    return without_sc\n\ndata[column] = data[column].progress_apply(lambda text: remove_single_char(str(text)))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:34.910006Z","iopub.execute_input":"2022-12-30T10:48:34.910516Z","iopub.status.idle":"2022-12-30T10:48:35.000293Z","shell.execute_reply.started":"2022-12-30T10:48:34.910474Z","shell.execute_reply":"2022-12-30T10:48:34.999188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_most_frequent_words(data,column,frequent_words_count):\n    all_text = ' '.join(data[column].tolist())\n    cnt = Counter(all_text.split())\n    most_frequent_words = cnt.most_common(frequent_words_count)\n    # print(most_frequent_words)\n    data[column] = data[column].progress_apply(lambda text: \" \".join(list(set(text.split()) - set(most_frequent_words))))\n    return data\n\ndef remove_rare_words(data,column,rare_words_count):\n    all_text = ' '.join(data[column].tolist())\n    cnt = Counter(all_text.split())\n    most_frequent_words = cnt.most_common()\n    rare_words = most_frequent_words[len(most_frequent_words)-rare_words_count:]\n    # print(rare_words)\n    rare_words = set(dict(rare_words).keys())\n\n    data[column] = data[column].progress_apply(lambda text: \" \".join(list(set(text.split()) - rare_words)))\n    return data\n\ndata = remove_most_frequent_words(data,column='Text',frequent_words_count=20)\ndata = remove_rare_words(data,column=\"Text\",rare_words_count=1000)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:35.001786Z","iopub.execute_input":"2022-12-30T10:48:35.002321Z","iopub.status.idle":"2022-12-30T10:48:35.298338Z","shell.execute_reply.started":"2022-12-30T10:48:35.002276Z","shell.execute_reply":"2022-12-30T10:48:35.297193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Stemming\n\n# column = 'Text'\n# stemmer = PorterStemmer()\n# def stem_words(text):\n#     return \" \".join([stemmer.stem(word) for word in text.split()])\n# data[column] = data[column].apply(lambda text: stem_words(text))\n# data.head()\n\n# Lemmatization \n\ncolumn = \"Text\"\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\ndata[column] = data[column].progress_apply(lambda text: lemmatize_words(text))\ndata","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:35.299589Z","iopub.execute_input":"2022-12-30T10:48:35.299953Z","iopub.status.idle":"2022-12-30T10:48:51.891152Z","shell.execute_reply.started":"2022-12-30T10:48:35.299922Z","shell.execute_reply":"2022-12-30T10:48:51.889641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from textblob import TextBlob\n\n# def correct_spellings(text):\n#     return str(TextBlob(text).correct())\n\n# i =1000\n# print(data['Text'][i])\n# print(\"-------------------------------------\")\n# print(correct_spellings(data['Text'][i]))\n\n# !pip install pyspellchecker\n# from spellchecker import SpellChecker\n# spell = SpellChecker()\n# def correct_spellings(text):\n#     corrected_text = []\n#     misspelled_words = spell.unknown(text.split())\n#     for word in text.split():\n#         if word in misspelled_words:\n#             corrected_text.append(str(spell.correction(word) or word ))\n#         else:\n#             corrected_text.append(word)\n#     return \" \".join(corrected_text)\n\n# column = \"Text\"\n# data[column] = data[column].apply(lambda text: correct_spellings(str(text)))\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:51.892817Z","iopub.execute_input":"2022-12-30T10:48:51.893959Z","iopub.status.idle":"2022-12-30T10:48:51.900168Z","shell.execute_reply.started":"2022-12-30T10:48:51.893911Z","shell.execute_reply":"2022-12-30T10:48:51.898749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data['oh_label'].unique()\n# data['oh_label'].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:51.901758Z","iopub.execute_input":"2022-12-30T10:48:51.902451Z","iopub.status.idle":"2022-12-30T10:48:51.915244Z","shell.execute_reply.started":"2022-12-30T10:48:51.902402Z","shell.execute_reply":"2022-12-30T10:48:51.914048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['oh_label'] = data['oh_label'].astype(int)\n\nindex = []\nfor i in tqdm(list(data.index)):\n    count = len(data['Text'][i].split())\n    if (count<=3) & (data['oh_label'][i]==0):\n        index.append(i)\n        \ndata.drop(index=index,inplace=True) ","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:51.931542Z","iopub.execute_input":"2022-12-30T10:48:51.932426Z","iopub.status.idle":"2022-12-30T10:48:52.261204Z","shell.execute_reply.started":"2022-12-30T10:48:51.93238Z","shell.execute_reply":"2022-12-30T10:48:52.260003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.to_csv(\"preprocessed_text_twitter_parsed_dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:52.26275Z","iopub.execute_input":"2022-12-30T10:48:52.263096Z","iopub.status.idle":"2022-12-30T10:48:52.414165Z","shell.execute_reply.started":"2022-12-30T10:48:52.263068Z","shell.execute_reply":"2022-12-30T10:48:52.41291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/working/preprocessed_text_twitter_parsed_dataset.csv\")\ndata.drop(columns=\"Unnamed: 0\",inplace=True)\ndata.dropna(axis=\"index\",inplace=True)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:48:52.415775Z","iopub.execute_input":"2022-12-30T10:48:52.41626Z","iopub.status.idle":"2022-12-30T10:48:52.505351Z","shell.execute_reply.started":"2022-12-30T10:48:52.416208Z","shell.execute_reply":"2022-12-30T10:48:52.504096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nimport re\nimport nltk\nnltk.download('omw-1.4')\nimport spacy\nimport string\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import word_tokenize,sent_tokenize  # tokenizing\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nimport nltk\nfrom nltk.stem import PorterStemmer\n#stop-words\nstop_words=set(nltk.corpus.stopwords.words('english'))\n# Graphs\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\n\n# vectorizers for creating the document-term-matrix (DTM)\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.metrics import accuracy_score,roc_auc_score,f1_score,classification_report,precision_score, recall_score\nfrom mlxtend.plotting import plot_confusion_matrix\n# ML Models\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n#keras #tf\nimport tensorflow as tf\nimport keras\nfrom keras.preprocessing.text import one_hot,Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM, BatchNormalization, Dropout, InputLayer, ReLU\nfrom keras.initializers import Constant\nfrom keras.callbacks import EarlyStopping\nimport keras_tuner as kt\nimport tensorflow_addons as tfa\n\n# F1_Score_tf = tfa.metrics.F1Score(num_classes=1)\n# from sklearn.metrics import f1_score\n# F1_Score_tf = f1_score\nfrom keras import backend as K\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef F1_Score_tf(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\nauc_roc_metric_ROC = keras.metrics.AUC(curve='ROC')\n\n\n#gensim w2v  #word2vec\nimport gensim\nfrom gensim.models import Word2Vec,KeyedVectors\nfrom nltk.tokenize import sent_tokenize, word_tokenize","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:53:07.214088Z","iopub.execute_input":"2022-12-30T10:53:07.21453Z","iopub.status.idle":"2022-12-30T10:53:07.404433Z","shell.execute_reply.started":"2022-12-30T10:53:07.214493Z","shell.execute_reply":"2022-12-30T10:53:07.401255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ML Models","metadata":{}},{"cell_type":"code","source":"# Plotting Loss vs epochs graph, Accuaracy vs epochs graph\ndef Plot_Loss_Accuracy( history, epochs ):\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    auc = history.history['accuracy']\n    val_auc = history.history['val_accuracy']\n\n    epochs = np.linspace(1,epochs,epochs)\n    epochs\n\n    fig = go.Figure(data=go.Scatter( x=epochs, y=loss, name='Loss' ))\n    fig.add_trace( go.Scatter( x=epochs, y=val_loss, name='Validation Lass' ) )\n    fig.layout.update( xaxis_title = 'No. of Epochs',\n                       yaxis_title = \"Loss\",\n                       legend=dict( yanchor=\"bottom\", y=0.01, xanchor=\"right\", x=0.99),\n                       width=750, height=650,\n                       font=dict(size=18))\n    fig.show()\n\n    fig = go.Figure(data=go.Scatter( x=epochs, y=auc, name='Accuracy Score' ))\n    fig.add_trace( go.Scatter( x=epochs, y=val_auc, name='Validation Accuracy Score' ) )\n    fig.layout.update( xaxis_title = 'No. of Epochs',\n                       yaxis_title = \"Accuracy Scores\",\n                       legend=dict( yanchor=\"bottom\", y=0.01, xanchor=\"right\", x=0.99),\n                       width=750, height=650,\n                       font=dict(size=18))\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:53:07.407215Z","iopub.execute_input":"2022-12-30T10:53:07.407651Z","iopub.status.idle":"2022-12-30T10:53:07.421767Z","shell.execute_reply.started":"2022-12-30T10:53:07.407609Z","shell.execute_reply":"2022-12-30T10:53:07.420367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Evaluation(model,X_train,X_test,y_train,y_test,hypertuning=False):\n\n  plt.figure(  figsize=(12,6) )\n #print( \"-----------------------------------------------------------------------------------------------------------\")\n # print( model )\n # print( \" For Train Set :  \")\n  y_pred = model.predict(X_train)\n  y_pred_proba = model.predict_proba(X_train)\n\n  accuracy_train = accuracy_score( y_train, y_pred )\n  precision_train = precision_score( y_train, y_pred )\n  recall_train = recall_score(y_train, y_pred)\n  F1_score_train = f1_score(y_train, y_pred)\n  #print(\"F1_Score = \", F1_score_train )\n  roc_auc_train = roc_auc_score(y_train, y_pred_proba[:,1])\n  #print( classification_report( y_train, y_pred ) )\n\n  #print( \" For Test Set :  \")\n  y_pred = model.predict(X_test)\n  y_pred_proba = model.predict_proba(X_test)\n\n  accuracy_test = accuracy_score( y_test, y_pred )\n  precision_test = precision_score( y_test, y_pred )\n  recall_test = recall_score(y_test, y_pred)\n  F1_score_test = f1_score(y_test, y_pred)\n  #print(\"F1_Score = \", F1_score_test )\n  roc_auc_test = roc_auc_score(y_test, y_pred_proba[:,1])\n  #print( classification_report( y_test, y_pred ) )\n\n  #print('------------------------------------------------------------------------------------------------------------')\n  #print(\"\\n\")\n\n#   fpr, tpr, thresh = roc_curve( y_test, y_pred )\n#   plt.plot( fpr, tpr, label=model + str( auc_score ) ) \n  return  accuracy_train, precision_train, recall_train, F1_score_train, roc_auc_train, accuracy_test, precision_test, recall_test, F1_score_test, roc_auc_test","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:53:07.423601Z","iopub.execute_input":"2022-12-30T10:53:07.427441Z","iopub.status.idle":"2022-12-30T10:53:07.439388Z","shell.execute_reply.started":"2022-12-30T10:53:07.4274Z","shell.execute_reply":"2022-12-30T10:53:07.438488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_models_with_default_paramters(X_train,X_test,y_train,y_test):\n  models_default = [ XGBClassifier(), \n                     GaussianNB(), #MultinomialNB()\n                     RandomForestClassifier(),\n                     DecisionTreeClassifier(), \n                     # ExtraTreeClassifier(), KNeighborsClassifier(),\n                     # SVC(probability=True),\n                     AdaBoostClassifier(),\n                     ]\n\n  \n  F1_Score_train = []\n  Accuracy_train = []\n  Recall_train = []\n  Precision_train = []\n  ROC_AUC_Score_train = []\n\n\n  F1_Score_test = []\n  Accuracy_test = []\n  Recall_test = []\n  Precision_test = []\n  ROC_AUC_Score_test = []\n\n  Model_Name = []\n\n  for model in models_default:\n    # print(model)\n    Model_Name.append( model )\n    model.fit(X_train, y_train)\n\n    accuracy_train, precision_train, recall_train, F1_score_train, roc_auc_train, accuracy_test, precision_test, recall_test, F1_score_test, roc_auc_test = Evaluation(model,X_train,X_test,y_train,y_test,False)\n    \n    F1_Score_train.append( F1_score_train )\n    Accuracy_train.append( accuracy_train )\n    Recall_train.append( recall_train )\n    Precision_train.append( precision_train )\n    ROC_AUC_Score_train.append( roc_auc_train )\n\n    F1_Score_test.append( F1_score_test )\n    Accuracy_test.append( accuracy_test )\n    Recall_test.append( recall_test )\n    Precision_test.append( precision_test )\n    ROC_AUC_Score_test.append( roc_auc_test )\n    \n  results = pd.DataFrame()\n  results['Model_Name'] = Model_Name\n    \n  train_test_f1_score_difference = np.subtract(F1_Score_train,F1_Score_test)  # To Check Overfitting/Underfitting\n\n  results['F1_Score on Test Set'] = F1_Score_test\n  results['Accuracy on Test Set'] = Accuracy_test\n  results['Recall on Test Set'] = Recall_test\n  results['Precision on Test Set'] = Precision_test\n  results['ROC_AUC_Score on Test Set'] = ROC_AUC_Score_test\n\n  results['F1_Score on Train Set'] = F1_Score_train\n  results['Accuracy on Train Set'] = Accuracy_train\n  results['Recall on Train Set'] = Recall_train\n  results['Precision on Train Set'] = Precision_train\n  results['ROC_AUC_Score on Train Set'] = ROC_AUC_Score_train\n\n  results['Difference of F1_Score on train and test'] = train_test_f1_score_difference\n\n  results = results.sort_values(by=['F1_Score on Test Set','Difference of F1_Score on train and test'],ascending = [False, True]) \n\n  return results","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:53:07.442564Z","iopub.execute_input":"2022-12-30T10:53:07.443092Z","iopub.status.idle":"2022-12-30T10:53:07.459199Z","shell.execute_reply.started":"2022-12-30T10:53:07.443042Z","shell.execute_reply":"2022-12-30T10:53:07.458061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***","metadata":{}},{"cell_type":"markdown","source":"# Target","metadata":{}},{"cell_type":"markdown","source":"Target is slightly imbalanced","metadata":{}},{"cell_type":"code","source":"fig = px.pie(values=[sum(data['oh_label']==0),sum(data['oh_label']==1)], names=['no(0)','yes(1)'] , title='Target')\nfig.update_layout(width=300,height=300)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:53:07.460907Z","iopub.execute_input":"2022-12-30T10:53:07.462128Z","iopub.status.idle":"2022-12-30T10:53:08.377819Z","shell.execute_reply.started":"2022-12-30T10:53:07.46208Z","shell.execute_reply":"2022-12-30T10:53:08.375418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization and Applying Algorithms","metadata":{}},{"cell_type":"code","source":"# def create_corpus(data, column, vocabulary=True):\n#     corpus = \"\"\n#     for i in tqdm(range(data.shape[0])):\n#             corpus = corpus + \" \" + data[column][i]\n#     if vocabulary==True:\n#         corpus = set(corpus.split())\n#     if vocabulary==False:\n#         pass\n#     return corpus\n# corpus = create_corpus(data,\"Text\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:53:08.37911Z","iopub.execute_input":"2022-12-30T10:53:08.379442Z","iopub.status.idle":"2022-12-30T10:53:08.385326Z","shell.execute_reply.started":"2022-12-30T10:53:08.379413Z","shell.execute_reply":"2022-12-30T10:53:08.384023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF - ML","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/code/marchputt/nlp-text-classification","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/working/preprocessed_text_twitter_parsed_dataset.csv\")\ndata.drop(columns=\"Unnamed: 0\",inplace=True)\ndata.dropna(axis=\"index\",inplace=True)\ndata\n\n# # # Taking sample temporary sample of data for writting code\n# from sklearn.model_selection import train_test_split\n# data_bigger, data  = train_test_split(data, test_size=0.001, random_state=10, stratify=data['oh_label'])\n# data.reset_index(inplace=True)\n# data.drop(columns=['index'],inplace=True)\n# data\n\nX_train, X_test, y_train, y_test = train_test_split(data.drop(columns=[\"oh_label\"]), data[['oh_label']], test_size=0.2, random_state=10, stratify=data['oh_label'])\nX_train.reset_index( inplace=True, drop=True ), X_test.reset_index( inplace=True, drop=True ), y_train.reset_index( inplace=True, drop=True ), y_test.reset_index( inplace=True, drop=True )","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:53:08.387256Z","iopub.execute_input":"2022-12-30T10:53:08.387637Z","iopub.status.idle":"2022-12-30T10:53:08.4861Z","shell.execute_reply.started":"2022-12-30T10:53:08.387587Z","shell.execute_reply":"2022-12-30T10:53:08.485175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer( lowercase=True, ngram_range=(1, 1), max_features=300, \n                               binary=False, stop_words = 'english' )\n\nX_train_text = pd.DataFrame.sparse.from_spmatrix( vectorizer.fit_transform(X_train['Text']) )\nX_test_text = pd.DataFrame.sparse.from_spmatrix( vectorizer.transform(X_test['Text']) )\n# vectorizer.get_feature_names_out()\n\nX_train = pd.concat([X_train_text,X_train.drop(columns=['Text'])], axis=1)\nX_test = pd.concat([X_test_text,X_test.drop(columns=['Text'])],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:53:08.487385Z","iopub.execute_input":"2022-12-30T10:53:08.488116Z","iopub.status.idle":"2022-12-30T10:53:08.735672Z","shell.execute_reply.started":"2022-12-30T10:53:08.488083Z","shell.execute_reply":"2022-12-30T10:53:08.734711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Results = apply_models_with_default_paramters(X_train,X_test,y_train,y_test)\nResults","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:53:08.739376Z","iopub.execute_input":"2022-12-30T10:53:08.739809Z","iopub.status.idle":"2022-12-30T10:53:32.529483Z","shell.execute_reply.started":"2022-12-30T10:53:08.739774Z","shell.execute_reply":"2022-12-30T10:53:32.52845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Neural network\nmodel = Sequential()\nmodel.add(Dense(512, input_dim=X_train.shape[1], activation=tf.keras.layers.ReLU()))\nmodel.add(Dense(256, activation=tf.keras.layers.ReLU()))\nmodel.add(Dense(64, activation=tf.keras.layers.ReLU()))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy', F1_Score_tf])\nhistory = model.fit(X_train, y_train , validation_data=(X_test,y_test), epochs=20, batch_size=32)\n\nPlot_Loss_Accuracy( history, epochs=20 )","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:53:32.53469Z","iopub.execute_input":"2022-12-30T10:53:32.535204Z","iopub.status.idle":"2022-12-30T10:54:14.684568Z","shell.execute_reply.started":"2022-12-30T10:53:32.535167Z","shell.execute_reply":"2022-12-30T10:54:14.683359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Pre-trained Word2Vec Model","metadata":{}},{"cell_type":"markdown","source":"https://stackoverflow.com/questions/46433778/import-googlenews-vectors-negative300-bin","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/working/preprocessed_text_twitter_parsed_dataset.csv\")\ndata.drop(columns=\"Unnamed: 0\",inplace=True)\ndata.dropna(axis=\"index\",inplace=True)\ndata\n\n# # # Taking sample temporary sample of data for writting code\n# from sklearn.model_selection import train_test_split\n# data_bigger, data  = train_test_split(data, test_size=0.001, random_state=10, stratify=data['oh_label'])\n# data.reset_index(inplace=True)\n# data","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:14.685934Z","iopub.execute_input":"2022-12-30T10:54:14.68626Z","iopub.status.idle":"2022-12-30T10:54:14.771114Z","shell.execute_reply.started":"2022-12-30T10:54:14.686233Z","shell.execute_reply":"2022-12-30T10:54:14.770021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_pre_model = KeyedVectors.load_word2vec_format('/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin',binary=True,limit=100000)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:14.772991Z","iopub.execute_input":"2022-12-30T10:54:14.773447Z","iopub.status.idle":"2022-12-30T10:54:17.423955Z","shell.execute_reply.started":"2022-12-30T10:54:14.773404Z","shell.execute_reply":"2022-12-30T10:54:17.423076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = w2v_pre_model.key_to_index","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:17.425759Z","iopub.execute_input":"2022-12-30T10:54:17.426168Z","iopub.status.idle":"2022-12-30T10:54:17.432755Z","shell.execute_reply.started":"2022-12-30T10:54:17.426132Z","shell.execute_reply":"2022-12-30T10:54:17.431535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vec = w2v_pre_model['man']\n# print(vec)\nw2v_pre_model.most_similar('man')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:17.434543Z","iopub.execute_input":"2022-12-30T10:54:17.434909Z","iopub.status.idle":"2022-12-30T10:54:17.536371Z","shell.execute_reply.started":"2022-12-30T10:54:17.434866Z","shell.execute_reply":"2022-12-30T10:54:17.53517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(data['oh_label']==0).sum(), (data['oh_label']==1).sum()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:17.538054Z","iopub.execute_input":"2022-12-30T10:54:17.539424Z","iopub.status.idle":"2022-12-30T10:54:17.556935Z","shell.execute_reply.started":"2022-12-30T10:54:17.539372Z","shell.execute_reply":"2022-12-30T10:54:17.55498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def document_vector(text,model):\n    # remove out-of-vocabulary words\n    doc = [word for word in text if word in vocab]\n    if len(doc)==0:\n        mean = np.nan\n    else:\n        mean = np.mean(model[doc], axis=0)\n    return mean\n\n\n# document_vector(data['Text'][0],w2v_pre_model)\ndata['Text_Vec'] = data['Text'].apply( lambda text: document_vector(text.split(),w2v_pre_model))\ndata.dropna(inplace=True)\ndata.reset_index(inplace=True)\ndata.drop(columns=[\"index\"],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:17.55895Z","iopub.execute_input":"2022-12-30T10:54:17.560205Z","iopub.status.idle":"2022-12-30T10:54:18.363241Z","shell.execute_reply.started":"2022-12-30T10:54:17.560159Z","shell.execute_reply":"2022-12-30T10:54:18.362124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:18.364662Z","iopub.execute_input":"2022-12-30T10:54:18.365135Z","iopub.status.idle":"2022-12-30T10:54:18.392756Z","shell.execute_reply.started":"2022-12-30T10:54:18.365091Z","shell.execute_reply":"2022-12-30T10:54:18.391427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(data['oh_label']==0).sum(), (data['oh_label']==1).sum()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:18.394433Z","iopub.execute_input":"2022-12-30T10:54:18.39491Z","iopub.status.idle":"2022-12-30T10:54:18.407491Z","shell.execute_reply.started":"2022-12-30T10:54:18.394873Z","shell.execute_reply":"2022-12-30T10:54:18.406376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(np.zeros(shape=(data.shape[0],300)),columns=[\"col_\"+str(i) for i in range(300)])\n\nfor i in range(data.shape[0]):\n    try:\n        df.iloc[i]=data['Text_Vec'][i]# .append(data['oh_label'][i])\n    except:\n        df.iloc[i] = [np.nan for i in range(300)]\n         \ndf['target'] = data[\"oh_label\"]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:18.409325Z","iopub.execute_input":"2022-12-30T10:54:18.409734Z","iopub.status.idle":"2022-12-30T10:54:19.408945Z","shell.execute_reply.started":"2022-12-30T10:54:18.409704Z","shell.execute_reply":"2022-12-30T10:54:19.407563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.isna().sum().sum()\n# (df['target']==0).sum(), (df['target']==1).sum()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:19.410734Z","iopub.execute_input":"2022-12-30T10:54:19.411247Z","iopub.status.idle":"2022-12-30T10:54:19.417464Z","shell.execute_reply.started":"2022-12-30T10:54:19.411201Z","shell.execute_reply":"2022-12-30T10:54:19.416209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(df.drop(columns=['target']),df[['target']],test_size=0.2,random_state=10,stratify=df[['target']])\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:19.419533Z","iopub.execute_input":"2022-12-30T10:54:19.419969Z","iopub.status.idle":"2022-12-30T10:54:19.539122Z","shell.execute_reply.started":"2022-12-30T10:54:19.419933Z","shell.execute_reply":"2022-12-30T10:54:19.538017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Results = apply_models_with_default_paramters(X_train,X_test,y_train,y_test)\nResults","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:54:19.540765Z","iopub.execute_input":"2022-12-30T10:54:19.541246Z","iopub.status.idle":"2022-12-30T10:55:58.276701Z","shell.execute_reply.started":"2022-12-30T10:54:19.541202Z","shell.execute_reply":"2022-12-30T10:55:58.275587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Neural network\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\nmodel.add(Dense(32, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy', auc_roc_metric_ROC, F1_Score_tf])\nhistory = model.fit(X_train, y_train , validation_data=(X_test,y_test), epochs=10, batch_size=32)\n\nPlot_Loss_Accuracy( history, epochs=10 )","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:55:58.278425Z","iopub.execute_input":"2022-12-30T10:55:58.278768Z","iopub.status.idle":"2022-12-30T10:56:14.440285Z","shell.execute_reply.started":"2022-12-30T10:55:58.278738Z","shell.execute_reply":"2022-12-30T10:56:14.43909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word2Vec Model","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/working/preprocessed_text_twitter_parsed_dataset.csv\")\ndata.drop(columns=\"Unnamed: 0\",inplace=True)\ndata.dropna(axis=\"index\",inplace=True)\ndata\n\n# # # Taking sample temporary sample of data for writting code\n\n# from sklearn.model_selection import train_test_split\n# data_bigger, data  = train_test_split(data, test_size=0.001, random_state=10, stratify=data['oh_label'])\n# data.reset_index(inplace=True)\n# data.drop(columns=['index'],inplace=True)\n# data","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:56:14.441746Z","iopub.execute_input":"2022-12-30T10:56:14.442145Z","iopub.status.idle":"2022-12-30T10:56:14.519171Z","shell.execute_reply.started":"2022-12-30T10:56:14.442111Z","shell.execute_reply":"2022-12-30T10:56:14.518014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data['Text'] = data['Text'].apply( lambda text: text.split() )\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:56:14.521199Z","iopub.execute_input":"2022-12-30T10:56:14.521681Z","iopub.status.idle":"2022-12-30T10:56:14.526957Z","shell.execute_reply.started":"2022-12-30T10:56:14.521634Z","shell.execute_reply":"2022-12-30T10:56:14.525647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 300\nsentences_list = data['Text'].to_list()\n# sentences_list[0:3]\nw2v_model = gensim.models.Word2Vec(sentences=sentences_list,window=5,min_count=1,vector_size=embed_dim)\nw2v_model.train(sentences_list,epochs=25,total_examples=len(sentences_list))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:56:14.528966Z","iopub.execute_input":"2022-12-30T10:56:14.529428Z","iopub.status.idle":"2022-12-30T10:56:22.848784Z","shell.execute_reply.started":"2022-12-30T10:56:14.529384Z","shell.execute_reply":"2022-12-30T10:56:22.847949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # embedding of a particular word.\n# w2v_model.wv.get_vector('like')\n\n# total numberof extracted words.\n# vocab = w2v_model.wv\nvocab_len = w2v_model.wv.vectors.shape[0]\nprint(\"The total number of words are : \", vocab_len )\n\n# # words most similar to a given word.\n# w2v_model.wv.most_similar('like')\n\n# # similaraity b/w two words\n# w2v_model.wv.similarity('good','like')\n\nvocab = w2v_model.wv.key_to_index.keys()\n# vocab\n\n#### https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:56:22.849915Z","iopub.execute_input":"2022-12-30T10:56:22.850379Z","iopub.status.idle":"2022-12-30T10:56:22.857244Z","shell.execute_reply.started":"2022-12-30T10:56:22.850332Z","shell.execute_reply":"2022-12-30T10:56:22.856062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def document_vector(text,model):\n    # remove out-of-vocabulary words\n    doc = [word for word in text if word in vocab]\n    if len(doc)==0:\n        mean = np.nan\n    else:\n        mean = np.mean(model.wv[doc], axis=0)\n    return mean\n\n\n# document_vector(data['Text'][0],w2v_pre_model)\ndata['Text_Vec'] = data['Text'].apply( lambda text: document_vector(text.split(),w2v_model))\ndata.dropna(inplace=True)\ndata.reset_index(inplace=True)\ndata.drop(columns=[\"index\"],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:56:22.866249Z","iopub.execute_input":"2022-12-30T10:56:22.866631Z","iopub.status.idle":"2022-12-30T10:56:22.952735Z","shell.execute_reply.started":"2022-12-30T10:56:22.866598Z","shell.execute_reply":"2022-12-30T10:56:22.951455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(np.zeros(shape=(data.shape[0],300)),columns=[\"col_\"+str(i) for i in range(300)])\n\nfor i in range(data.shape[0]):\n    try:\n        df.iloc[i]=data['Text_Vec'][i]# .append(data['oh_label'][i])\n    except:\n        df.iloc[i] = [np.nan for i in range(300)]\n         \ndf['target'] = data[\"oh_label\"]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:56:22.95435Z","iopub.execute_input":"2022-12-30T10:56:22.954985Z","iopub.status.idle":"2022-12-30T10:56:23.058403Z","shell.execute_reply.started":"2022-12-30T10:56:22.954951Z","shell.execute_reply":"2022-12-30T10:56:23.057267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(df.drop(columns=['target']),df['target'],test_size=0.2,random_state=10,stratify=df['target'])\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:56:23.059906Z","iopub.execute_input":"2022-12-30T10:56:23.060758Z","iopub.status.idle":"2022-12-30T10:56:23.074433Z","shell.execute_reply.started":"2022-12-30T10:56:23.060724Z","shell.execute_reply":"2022-12-30T10:56:23.073186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Results = apply_models_with_default_paramters(X_train,X_test,y_train,y_test)\nResults","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:56:23.075935Z","iopub.execute_input":"2022-12-30T10:56:23.077076Z","iopub.status.idle":"2022-12-30T10:56:25.073722Z","shell.execute_reply.started":"2022-12-30T10:56:23.077027Z","shell.execute_reply":"2022-12-30T10:56:25.072922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Neural network\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\nmodel.add(Dense(32, input_dim=X_train.shape[1], activation=tf.keras.layers.LeakyReLU(alpha=0.05)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy' , optimizer='adam', metrics=['accuracy', auc_roc_metric_ROC])\nhistory = model.fit(X_train, y_train , validation_data=(X_test,y_test), epochs=10, batch_size=32)\n\nPlot_Loss_Accuracy( history, epochs=10 )","metadata":{"execution":{"iopub.status.busy":"2022-12-30T10:56:25.074979Z","iopub.execute_input":"2022-12-30T10:56:25.075297Z","iopub.status.idle":"2022-12-30T10:56:27.465342Z","shell.execute_reply.started":"2022-12-30T10:56:25.075269Z","shell.execute_reply":"2022-12-30T10:56:27.463505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding Layer | DL","metadata":{}},{"cell_type":"markdown","source":"Preparing the data for keras embedding layer\n","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/working/preprocessed_text_twitter_parsed_dataset.csv\")\ndata.drop(columns=\"Unnamed: 0\",inplace=True)\ndata.dropna(axis=\"index\",inplace=True)\ndata\n\n# # # Taking sample temporary sample of data for writting code\n\n# from sklearn.model_selection import train_test_split\n# data_bigger, data  = train_test_split(data, test_size=0.001, random_state=10, stratify=data['oh_label'])\n# data.reset_index(inplace=True)\n# data.drop(columns=['index'],inplace=True)\n# data","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:12:08.585041Z","iopub.execute_input":"2022-12-30T11:12:08.585457Z","iopub.status.idle":"2022-12-30T11:12:08.666132Z","shell.execute_reply.started":"2022-12-30T11:12:08.585425Z","shell.execute_reply":"2022-12-30T11:12:08.664943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok = Tokenizer()\ntok.fit_on_texts(data['Text'])\nvocab_size = len(tok.word_index) + 1\nprint(\"Vocabulary Size: \", vocab_size )","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:12:08.81577Z","iopub.execute_input":"2022-12-30T11:12:08.81673Z","iopub.status.idle":"2022-12-30T11:12:09.101153Z","shell.execute_reply.started":"2022-12-30T11:12:08.81669Z","shell.execute_reply":"2022-12-30T11:12:09.099798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_sequence_and_padding(tokenizer, data, column):\n    sequences = tokenizer.texts_to_sequences(data[column]) # convert words into sequences\n    max_length = int(max([len(vector) for vector in sequences ])/4)\n    sequences = pad_sequences( sequences, maxlen=max_length, padding='post')\n    return sequences, max_length\n\nsequences, max_length = encode_sequence_and_padding(tok, data=data, column='Text')\nsequences = pd.DataFrame(sequences)\nprint(\"Maximum length of Sentence: \",max_length)\nsequences.to_csv(\"sequences.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:12:09.140481Z","iopub.execute_input":"2022-12-30T11:12:09.140895Z","iopub.status.idle":"2022-12-30T11:12:09.441274Z","shell.execute_reply.started":"2022-12-30T11:12:09.140862Z","shell.execute_reply":"2022-12-30T11:12:09.439947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(sequences, data['oh_label'], test_size=0.20, random_state = 10, stratify=data['oh_label'])","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:12:09.443428Z","iopub.execute_input":"2022-12-30T11:12:09.444671Z","iopub.status.idle":"2022-12-30T11:12:09.45976Z","shell.execute_reply.started":"2022-12-30T11:12:09.444629Z","shell.execute_reply":"2022-12-30T11:12:09.458067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the model\nmodel = Sequential()\nmodel.add(Embedding( vocab_size, 300, input_length = max_length, mask_zero=True ))\nmodel.add(LSTM(256, return_sequences=True))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(LSTM(64))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy',auc_roc_metric_ROC, F1_Score_tf])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:12:10.108061Z","iopub.execute_input":"2022-12-30T11:12:10.108889Z","iopub.status.idle":"2022-12-30T11:12:12.594883Z","shell.execute_reply.started":"2022-12-30T11:12:10.108835Z","shell.execute_reply":"2022-12-30T11:12:12.593746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=5\nbatch_size=32\nhistory = model.fit(X_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(X_test,y_test))\nPlot_Loss_Accuracy( history, epochs=epochs )","metadata":{"execution":{"iopub.status.busy":"2022-12-30T11:12:12.597072Z","iopub.execute_input":"2022-12-30T11:12:12.597399Z","iopub.status.idle":"2022-12-30T11:14:36.449045Z","shell.execute_reply.started":"2022-12-30T11:12:12.597372Z","shell.execute_reply":"2022-12-30T11:14:36.448091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}